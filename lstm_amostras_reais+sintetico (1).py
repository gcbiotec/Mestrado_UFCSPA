# -*- coding: utf-8 -*-
"""LSTM_Amostras_Reais+Sintetico.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WI3kk8seVxwpFMxA-ugKSL218B6SIoxX
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

amostras_excel = r"/content/Planilha_EHR.xlsx"
amostras_df = pd.read_excel(amostras_excel)
print(amostras_df)

amostras_sinteticas = r"/content/Amostras__Sinteticas.xlsx"
amostras_sint = pd.read_excel(amostras_sinteticas)
print(amostras_sint)

# Stack (concatenate) the dataframes vertically
amostras_df = pd.concat([amostras_df, amostras_sint], axis=0)

# Reset the index of the resulting dataframe
amostras_df.reset_index(drop=True, inplace=True)

print(amostras_df)

def remove_names_from_dataframe(csv_file_path, dataframe, column_name):
    try:
        # Read the CSV file into a list of names
        with open(csv_file_path, 'r') as file:
            names_to_remove = [line.strip() for line in file]

        # Check if the specified column exists in the DataFrame
        if column_name not in dataframe.columns:
            raise ValueError(f"Column '{column_name}' not found in the DataFrame.")

        # Remove rows containing names from the DataFrame
        dataframe = dataframe[~dataframe[column_name].isin(names_to_remove)]

        return dataframe
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None

nomes_masc = '/content/ibge-mas-10000.csv'
nomes_fem = '/content/ibge-fem-10000.csv'

Check_nomesMasc = remove_names_from_dataframe(nomes_masc,amostras_df,'EHR')
print(Check_nomesMasc)

Check_nomesFem = remove_names_from_dataframe(nomes_fem,amostras_df,'EHR')
print(Check_nomesFem)

pattern = r'\(\d+\)'
pattern_evol = r"[,;.:#-+/)(\d+>]"
amostras_df['CID'] = amostras_df['CID'].str.replace(pattern, '')
amostras_df['EHR'] = amostras_df['EHR'].str.replace(pattern_evol, '')

amostras_df['EHR'] = amostras_df['EHR'].str.lower()
amostras_df['EHR'] = amostras_df['EHR'].str.replace('[^a-zA-Z0-9]', ' ')
amostras_df['EHR'] = amostras_df['EHR'].str.replace('queixa principal: o paciente apresentou-se na unidade de saúde', ' ')

"""# Removendo StopWords:"""

!pip install nltk
nltk.download('stopwords')
stopWordNLTK = nltk.corpus.stopwords.words('portuguese')

def remove_stop_words(df):
    evolucao = df['EHR']
    cid = df['CID']

    if pd.isna(evolucao) or evolucao.strip() == '':
        return pd.Series([None, cid], index=['EHR', 'CID'])

    semStopWord = ' '.join([p for p in evolucao.split() if p not in stopWordNLTK])
    return pd.Series([semStopWord, cid], index=['EHR', 'CID'])

amostras_df = amostras_df.apply(remove_stop_words, axis=1)

"""# Aplicando Stemmer:"""

from nltk.stem import RSLPStemmer
nltk.download('rslp')

def aplicaStemmer(df):
    stemmer = nltk.stem.RSLPStemmer()
    evolucao = df['EHR']
    cid = df['CID']

    evolComStem = ' '.join([str(stemmer.stem(p)) for p in evolucao.split()])
    return pd.Series([evolComStem, cid], index=['EHR', 'CID'])

amostras_df = amostras_df.apply(aplicaStemmer, axis=1)

"""# Filtrando amostras com 6 ou mais ocorrências:"""

cid_counts = amostras_df['CID'].value_counts()
# Filter the dataframe to select only rows that have 'CID' column with values that appear 6 times or more
df_filtered = amostras_df[amostras_df['CID'].isin(cid_counts[cid_counts >= 6].index)]

print(df_filtered)

df_filtered['CID'].value_counts()

# Tokenize the text
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(df_filtered['EHR'])
text_sequences = tokenizer.texts_to_sequences(df_filtered['EHR'])

print(text_sequences)

# Pad the sequences to a fixed length
max_len = 16516
padded_sequences = pad_sequences(text_sequences, maxlen=max_len)

print(padded_sequences)

le = LabelEncoder()
class_labels = le.fit_transform(df_filtered['CID'])

print(class_labels)

# Get the lengths of all the texts in the 'EHR' column
ehr_lengths = df_filtered['EHR'].str.len()

# Find the longest text length
longest_ehr_length = ehr_lengths.max()

# Print the longest text length
print(longest_ehr_length)

unique_tokens = set()

# Iterate over all the texts in the 'EHR' column and add each token to the set
for text in df_filtered['EHR']:
    tokens = text.split()
    unique_tokens.update(tokens)

# Print the number of unique tokens
print(len(unique_tokens))

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, class_labels, test_size=0.2)

y_train_encoded = to_categorical(y_train, num_classes=14)

# Embedding layer
embedding_layer = Embedding(input_dim=6000, output_dim=128, input_length=16516)

# LSTM layer
lstm_layer = LSTM(units=128, dropout=0.2)

# Dense layer
dense_layer = Dense(14, activation='softmax')

# Sequential model
model = Sequential()
model.add(embedding_layer)
model.add(lstm_layer)
model.add(dense_layer)

import tensorflow as tf

# Define custom metrics
def precision(y_true, y_pred):
    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))
    predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))
    return true_positives / (predicted_positives + tf.keras.backend.epsilon())

def recall(y_true, y_pred):
    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))
    actual_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))
    return true_positives / (actual_positives + tf.keras.backend.epsilon())

def f1_score(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall, f1_score])

# Train the model
model.fit(X_train, y_train_encoded, epochs=20)

y_pred = model.predict(X_test)

import numpy as np
# Classe predita:
# Iterate over the list of arrays
for prev in y_pred:
    # Find the index of the maximum value in the array
    idx = np.argmax(prev)
    print("Index of maximum value in array:", prev, "is", idx)